\addchap*{Abstract}
Recent years have seen a significant advancement in autonomous driving technology, which has great promise for improving both the efficiency and safety of transportation. However, maintaining autonomous vehicles' dependability is still a major problem. Autonomous driving vehicles distinguish the objects in the surroundings with the help of pre-trained neural network models and sensor data. Achieving high accuracy in the training of a deep neural model necessitates exposure to a substantial volume of meticulously annotated data devoid of bias. Generating LiDAR-based datasets for training deep neural networks entails considerably greater time investment and elevated personal and manual workload costs compared to other machine vision functions such as image datasets. \acrshort{sota} datasets like SemanticKITTI are publicly available to forward the development of \acrfull{had} system research in tasks like semantic segmentation, and object detection. However, because of the vast amount of interacting possibilities between the environment and objects, publicly accessible datasets such as SemanticKITTI do not suffice in providing the comprehensive range of scenarios necessary to thoroughly assess the functionality of \acrfull{had} systems within the realm of machine vision. Researchers and practitioners also decided on synthetic data from simulations. As a consequence of inadequacies in scenery assembly, sensor realism, and sensor fidelity, coupled with environmental variability and the modeling of dynamic objects, a persistent domain gap endures between simulated and real-world environments. Despite improvements in the performance of models, such as those used for object detection, these advancements are still deemed insufficient.

This thesis investigates methods to enhance the comprehensiveness of LiDAR test data within the scope of scenery assembly, particularly focusing on the interplay between \acrfull{vru}, such as pedestrians, and their surrounding background. The adopted approach entails extracting foreground objects as representative prototypes or clusters by leveraging the 3D geometric information of points obtained from a "source point cloud". Subsequently, this information is utilized to augment another point cloud, referred to as the target, by accurately situating the object within the target environment. This thesis introduces a method for recombining point clouds through a statistical approach aimed at diversifying the array of foreground objects, encompassing their appearance and positional relationship concerning the LiDAR sensor and the scene. These methodologies hold applicability in testing scenarios involving both real-world driving situations and virtual test environments, particularly in cases where LiDAR data is utilized. The utilization of 3D geometric information from points facilitates the extraction of prototypes from a source scene cloud. Subsequently, the extracted prototype is relocated to a specified target location within the target scene cloud. Following the surface reconstruction of the "transformed" prototype, the point cloud associated with the prototype is recalculated, considering its new position within the target scene cloud. Furthermore, shadow-casting computations are performed to assess the prototype's influence on the target location. Consequently, a novel scenario point cloud is generated through the recombination of the prototype extracted from a source scene cloud and positioned within the target scene cloud.
