\chapter{Discussion and Conclusion}
Novel point cloud scenarios within the context of highly autonomous driving systems were generated by injecting a point cloud object from a source scene cloud into a target cloud. By using the orientation of the prototype of the source scene, the prototype was transformed into a target location on the target point cloud. After the transformation, a new point cloud for the transformed prototype was calculated and the shadow casted by the new prototype cloud on the target scene was observed.
\section{Challenges and Improvements}
The project could be improved in various ways.
\begin{itemize}
    \item Extraction of the prototype from the source scene cloud is done by using surface variation only. Several other geometric features could be used to remove the false positives.
    \item Extracted prototype cloud needs to be rotated on the target scene based on the target location. This is because a lidar sensor captures only the surface information of an object that is facing toward the sensor. Surfaces facing in the opposite direction of lidar sensor information is not available as a laser is not hitting these surfaces.
    \item By scanning an object from a different angle view and by the point cloud registrations technique, a point cloud representing a complete hollow 3D object could be constructed. Such objects could be rotated in any way in the target location. This results in an increase in the variability of scenarios.
    \item The centroid of points from the remaining points in the region of interest on the source scene cloud after cropping the prototype is calculated, which is later used in the transformation of the prototype on the target scene. If the remaining points after filtering the prototype consist of points from the prototype or higher z-values than the ground plane, then the transformation would be wrong. Eg. the transformed prototype point cloud lies submerged by 1 meter below the target ground plane.
    \item Experiment is done on flat surfaces with point cloud. Improvements could be made in the future by adding support for non-planar surfaces.
    \item The Prototype could only be extracted if it is standing on the ground plane point cloud. There could be a scenario where a prototype could be very near to the lidar sensor where there is no ground plane point cloud. Because of the field of view of the lidar sensor, prototype top surface points and not the ground plane points or lower region of prototype points are visible. Methods need to be developed for extracting such objects as there is no ground plane point cloud to calculate a reference centroid for transformation.
    \item The Raycasting process depends on the original direction of the laser on the target cloud. If the points represented on the target scene cloud are erroneous, then the resulting raycasted points will be erroneous.
    \item Accurately representing the surface of the prototype cloud after surface reconstruction would result in a reduced error during raycasting and shadowcasting.
    \item Currently, manual selection of a region is done for the insertion of the extracted prototype in a target region of the target scene cloud. Finding an appropriate region for the insertion of the prototype on the target region of the target scene cloud using 3D geometric features like planarity, linearity, etc could remove the step for manual selection of region on the target scene cloud. If the manual selection of the target region step is eliminated then by using the pre-saved extracted prototype, real-time injection on the target scene cloud could be possible.
\end{itemize}

\section{Conclusion}
Thus, a novel scenario was synthesized through lidar data augmentation. This involved extracting a prototype cloud utilizing the geometric features of 3D points in the point cloud. The prototype surface was then reconstructed at a designated location within the target cloud, and new point clouds for the prototype were generated based on the original direction of laser rays in the target cloud. Subsequently, shadows were projected onto the target scene, resulting in the creation of a new point cloud representing the altered scene. Evaluation of the approach was conducted by analyzing two point clouds representing identical backgrounds: one with a ground truth prototype (foreground object) and another without. This methodology allowed for the examination of discrepancies between the ground truth and the calculated new cloud without the necessity of considering differing background point clouds. The approach explored in the thesis could be enhanced by refining the surface reconstruction step. It could potentially be integrated into a testing phase for evaluating the efficacy of \acrfull{had} systems.