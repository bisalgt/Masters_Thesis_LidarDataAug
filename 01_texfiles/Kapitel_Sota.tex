\chapter{Related Work}
The \acrfull{ecu} of the \acrshort{had} system are in charge of comprehending the environment and making the right decisions. To accomplish high-level tasks like course planning and collision avoidance, the \acrshort{had} system has to estimate a more detailed 3D bounding box within the real world. The fundamental part of the ECUs that manages perception, judgment, route planning, etc. is the  \acrfull{dnn}. The quantity and quality of annotated training data are the primary determinants of DNN performance. The main issue that arises while training a DNN based on point clouds is the scarcity of ground truths, which severely restricts the network's ability to converge quickly and perform at its best. More data is required for DNN to reduce the problem of class imbalance, enhance generalization, and prevent overfitting of the model.

One interesting approach that makes it simple to collect all ground truth labels is synthetic data simulation. Utilizing computer graphics techniques, efforts have been made to produce artificially labeled data \parencite{SuQLG15, 8462926, abs-1809-08495}.  Using the video game engine's APIs from Grand Theft Auto V (GTA-V), \parencite{abs-1804-00103} gathers the calibrated pictures and point cloud and uses this information for vehicle recognition in subsequent projects \parencite{8462926}. Simulators like AutonoVi-Sim, \acrshort{carla}  \parencite{8575306, Dosovitskiy17} are also employed for purely synthetic data generation. The \acrlong{cg} world has a restricted scale and complexity and is primarily created by hand. Although it is demonstrated that using these simulated data might enhance DNN performance, there are still certain issues like domain gaps\parencite{zero_domain_gap, care_real_and_syn_gap} that need to be resolved. 

\section{Data Augmentation}
\acrfull{da} has been extensively investigated for 2D images \parencite{transfsim2000, NIPS2000_ba9a56ce} and has shown enormous possibilities in training effective \acrshort{dnn} models with restricted training images. \acrshort{da} tries to modify and create new training samples in order to extend the variety of the training data \parencite{xiao2022polarmix}. \acrshort{da} has been a standard pre-processing step when training a neural network, as its efficacy has been demonstrated across a wide range of tasks \parencite{lidar_aug, second2018, pointmixup2020}. In \parencite{abs-1708-01566}, the real-world image dataset is enhanced by the addition of rendered automobiles, which benefits from the foreground's variance and the background's realism. For the 2D image domain, several data augmentation methods are covered. In Cutout \parencite{cutout}, sections from images are taken off to enhance an image patch. MixUp \parencite{mixup} combines two images and the corresponding labels. CutMix \parencite{cutmix} is a technique that replaces a part of an image with a patch from another image, combining Cutout and MixUp. Though many efforts have been made on image augmentation, very few concentrate on point cloud data augmentation.

\parencite{DBLP:journals/corr/abs-2004-01643} delves into a variety of augmentation strategies, such as global augmentation, local augmentation, GT-filtering, and GT-sampling, to enhance data diversity in 3D point clouds. While local augmentation methods are specific to the points associated with certain objects in the scene, global augmentation strategies work on the point cloud representing the whole scene.
Global augmentations are applied concurrently to every point in the point clouds and every annotation. Several methods, including global translation, rotation, scaling, random flipping, and ground removal, are included in global augmentation. In contrast to global augmentation, local augmentation techniques also involve translation, rotation, and scaling. However, the key distinction lies in their application: rather than applying these transformations to every point indiscriminately, they are only used with specific labels and the points that are included in those labels. By using distinct random values for every label class, this method guarantees that each label class is enhanced individualistically from the others.
The GT-Filtering method serves as a means to filter ground truth objects. This filtering process can be contingent upon various factors, such as difficulty level (difficulty level as described in KITTI \parencite{Geiger2012CVPR}) or the extent of points contained within the objects. The practice of incorporating more reference instances, such as individuals, from other scenes into the present scene is known as GT-Sampling. In order to do this, a database linking class labels to points must be created and all labels must be reprised over once. This strategy is used to rectify the disparity between the "background" and "foreground" in datasets like SemanticKITTI \parencite{behley2019semantickitti}, thereby aiming for more balanced datasets. The subsequent section delves into various LiDAR augmentation strategies, exploring methods to enhance LiDAR data for improved performance in tasks such as object detection, segmentation, and classification.

\section{Augmentation Strategies involving LiDAR point cloud}

This section provides a detailed examination of different augmentation strategies concerning LiDAR point clouds. The emphasis is placed on elucidating the key features and characteristics of each strategy, delineating their significance in enhancing the quality and diversity of LiDAR data for various applications. 
\begin{itemize}
    \item \textbf{SECOND: Sparsely Embedded Convolution Detection} : In this study \parencite{second2018}, a database was constructed to store the labels of all reference (\acrshort{gt}) objects and their corresponding point cloud data derived from the training dataset's 3D bounding boxes. Throughout the learning process, a subset of ground truth objects was arbitrarily chosen from this repository using GT-Sampling methodology and integrated into the present training point cloud by using the join operation. This augmentation strategy aimed to augment the number of \acrshort{gt} reference objects per point cloud, thus facilitating the simulation of objects prevalent in varied surrounding conditions. Consequently, the detection performance was enhanced; nevertheless, it was observed that the methodology failed to consider the obstruction interaction that exists between various enhanced objects as well as among surrounding regions and enhanced objects, thereby disregarding the importance of rational placement and occlusion considerations. Overall, the augmentation technique primarily relied on the simple reuse of ground truth objects.
    \item \textbf{Mix3D} : In this methodology \parencite{mix3d2021}, several augmentation techniques is employed to enhance the diversity and robustness of the LiDAR point cloud data. Firstly, the translation of multiple scenes to the origin was carried out by subtracting the centroid from all point positions, ensuring uniformity in spatial orientation. Subsequently, random transformations such as flipping in both horizontal directions and random rotations along different axes is applied to introduce variability. Additionally, random sub-sampling, elastic distortion, and random scaling is implemented, along with possible contrast augmentation, random brightness adjustments, and color jittering. Post-augmentation, the augmented point clouds from both scenes is merged by simply combining their points, effectively forming the union of the two sets. Furthermore, to augment the data out-of-context, points from two scenes is concatenated, serving as a valuable augmentation strategy, particularly useful for tasks like semantic segmentation.
    \item \textbf{LiDAR-Aug} : This research endeavor \parencite{lidar_aug}, introduces a rendering-based framework specifically designed to augment LiDAR point cloud data. The automatic implementation of obstruction impediment is ensured by the seamless integration of supplemented elements into genuine background scenes through the utilization of rendering methodologies. Furthermore, a novel method is proposed for incorporating \acrfull{cad} models into scenes and generating augmented LiDAR point clouds using a rendering module. The framework also employs the lightweight "ValidMap" strategy to impose stances for the supplemented objects, ensuring collision avoidance and achieving realistic obstruction insertion. Moreover, improvements are made over GT sampling by implementing measures to avoid object collisions during the object insertion process.
    \item \textbf{Augmented LiDAR Simulator for Autonomous Driving} : This study \parencite{aug_lidar_sim_2020} presents a LiDAR point cloud simulation framework  designed to enrich real point clouds by incorporating synthetic obstacles. The synthetic obstacles, represented by movable \acrshort{cad} models, encompass various types, with 500 models dedicated to pedestrians, 45 models allocated for cars, SUVs, etc. Real-world background models acquired using the RIEGL scan device, renowned for its high-speed, high-performance dual scanner mobile mapping system capabilities, are seamlessly integrated with semantic information. Background annotation using semantic segmentation information obtained through PointNet++ \parencite{pointnetplus2017} is employed, with annotations meticulously refined through filtration by groups of annotators. Additionally, the framework incorporates realistic obstacle placement learned from real traffic scenes to ensure the authenticity and accuracy of the augmented point cloud environment.
    \item \textbf{LiDARsim} : In this simulation-based approach \parencite{lidarsim2020}, the scan background is substituted with a method involving registration techniques for LiDAR and diverse frames aggregation, while dynamic objects are regenerated out of \acrshort{pcd} data rather than relying on artificial \acrshort{cad} models. To facilitate this, a substantial catalog comprising 3D static maps and dynamic objects is first compiled through extensive drives across different cities with the self-driving fleet. Scenarios are subsequently generated by selecting scenes from this catalog and virtually placing a \acrfull{sdv} and dynamic objects within the chosen scenes, ensuring plausible and realistic arrangements.
    \item \textbf{PointMixup} : PointMixup \parencite{pointmixup2020} introduces an interpolation method influenced by MixUp \parencite{mixup} in image enhancement strategy. This approach discovers novel instances by efficiently assigning the path function between the labeled training \acrshort{pcd}s.
    \item \textbf{PointAugment} : This approach \parencite{pointaugment2020}is utilized for automatically optimizing and augmenting point cloud samples during the training of classification networks through the use of an augmentor to enhance data diversity. This method is employed to learn augmentation functions tailored to individual samples or classes, ensuring specificity in augmentation strategies. This approach integrates sample-aware auto-augmentation techniques and incorporates objectwise augmentor learning alongside classification learning using labeled training samples. However, the inclusion of an additional augmentor network and the intricate adversarial training process may introduce practical challenges to implementation.
    \item \textbf{PointPainting} : The proposed approach \parencite{pointpainting2019} involves initially executing the camera image through an image categorization algorithm. Subsequently, the anticipated categorization class grades are projected onto the LiDAR point cloud, effectively "painting" the point cloud with the segmentation results obtained from the camera image. This method offers an enhancement in 3D detection capabilities compared to the augmentation techniques.
    \item \textbf{PointAugmenting} : This research \parencite{pointaugmenting2021} introduces an object detection algorithm focusing on integrating 2D camera images with 3D LiDAR point clouds. In this methodology, deep features extracted by a 2D Object Detector from camera images are projected onto the corresponding points within the LiDAR point cloud. This process facilitates the fusion of information from the 2D image domain into the 3D LiDAR space, enhancing the feature representation and potentially improving object detection performance.
    \item \textbf{\acrfull{paaug}} : The proposed method \parencite{pa_aug2020} leverages the structural information of 3D ground-truth boxes to enable the network to learn intra-object relationships. It prolongs the GT-Sampling technique through creating sub-division within GT objects and introducing random augmentation to every division. Objects such as cars, cyclists, and pedestrians from the KITTI dataset are utilized for this purpose. By dividing objects into multiple partitions and probabilistically applying LiDAR data augmentation operations like dropout, swapping, mixing, sparsifying, and introducing noise to each partition region, the method not only enhances the accuracy of the dataset but also demonstrates robust performance on corrupted data.
    \item \textbf{Augmentation of LiDAR for Fog Simulation} : A novel fog simulation method suitable for any LiDAR dataset is discussed in \parencite{fog_sim_2021}, focusing on achieving physically valid simulations. This method involves augmenting clear weather point cloud data with artificial fog, thereby enhancing the realism of the dataset. The strategy has enhanced the efficacy of 3D object detection under challenging atmospheric conditions characterized by the presence of fog.
    \item \textbf{Augmentation of LiDAR for Snowfall Simulation} :\parencite{snow_sim_2022} discusses augmenting the LiDAR point cloud to simulate real-world snowfall. Using this method, artificial snow is added to clear weather point clouds. With this method, each LiDAR line's snow particle sample is taken in two dimensions, and the resulting geometry is used to modify the measurement for each LiDAR beam.
    \item \textbf{Patch-based Progressive 3D Point Set Upsampling} :  \parencite{patched_up_2018} discusses the method of upsampling from lower resolution points to enhance resolution, aiming to reveal detailed geometric structures from sparse and noisy inputs.
    \item \textbf{\acrfull{sessd}} : In the proposed methodology \parencite{sessd_2021}, the ground truth bounding box is partitioned into six pyramid shapes by connecting the centroid with the faces of the box. Subsequently, random dropout, swap, and sparsify operations are applied to each subset of points derived from these divisions, effectively simulating partial object occlusion, increasing object diversity, and mimicking variations in point sparsity. This process treats the divided point subsets as disassembled parts, allowing for targeted manipulation. Prior to shape-aware data augmentation, a series of global transformations are applied to the input point cloud, including random translation, flipping, and scaling. Overall, this approach integrates both shape-aware and global transformations to augment the data effectively.
    \item \textbf{\acrfull{ppba}} : PPBA \parencite{ppba_2020} uses automated data enhancement methods to find the ideal settings for augmentation. During this procedure, the augmentation schedule is optimized by reducing the size of the search space and using the most effective parameters from previous rounds. It is discussed that sampling from individual frames of data from sensors results in a tenfold gain in data efficiency.
    \item \textbf{\acrfull{dada}} :  \parencite{dada_2023} In this approach, transformation of orthogonal coordinates of the point cloud into a spherical coordinate system is carried out, followed by sampling based on the LiDAR characteristics specific to the dataset, such as Velodyne HDL-64E for KITTI \parencite{Geiger2012CVPR} or a 40-beam LiDAR for the ONCE dataset \parencite{mao2021one}. The object is evenly sliced and sampled in accordance with the LiDAR resolution corresponding to the dataset's LiDAR characteristics. In the case of existing ground truth (GT) objects, points are simulated while adhering to the LiDAR characteristics to maintain the object's shape. Subsequently, downsampling or upsampling is performed based on the point density requirements.
    \item \textbf{PolarMix} : This method \parencite{xiao2022polarmix} presents an innovative way to mix in cylindrical coordinates at the object and scene levels. Swapping at the scene level involves slicing along the azimuth axis to exchange point cloud regions of two circular LiDAR scans. Mixing at the instance level involves copying the rotated point objects into additional scans after cropping them from one LiDAR scan and rotating them by various azimuth degrees to make several copies. It should be noted, nevertheless, that this approach does not take shadowing effects into consideration and can lead to the illogical positioning of instance objects.
    \item \textbf{PatchAugment} : The focus of this methodology \parencite{patch_aug_2021} is on enhancing data inside the immediate area or neighborhood of the sampled object, which principle is similar to the \acrshort{paaug} data augmentation technique.
    \item \textbf{PointCutMix} : Inspired by \acrfull{msda} in the image domain, this method \parencite{pointcutmix_2021} generates new training data by mixing the original training samples. Specifically, subsets of point objects are replaced with those of other objects to enrich the training data. To utilize this approach, annotated point cloud objects from two point clouds with corresponding labels are required.
    \item \textbf{RS-Aug} : This approach \parencite{rs_aug_2023} involves constructing an augmented reality scene to enhance the diversity of the training dataset. Initially, a pre-trained semantic segmentation model, RangeNet++ \parencite{rangenetplus2019}, is utilized to estimate semantic labels. This step serves as a preprocessing stage for the creation of a database in the auto-annotation process. Subsequently, the database is sampled for rendering augmentation in the subsequent step.
    \item \textbf{\acrfull{br}} : In this approach \parencite{xu2022reality}, position-level annotation is employed to build a virtual scene using an existing object database, facilitating weakly supervised 3D object detection. This process entails labeling the center of each object in the 3D space, followed by generating a virtual scene from a repository of 3D shapes.
    \item \textbf{Object Insertion Based Data Augmentation for Semantic Segmentation} : In this research \parencite{9811816}, annotated LiDAR point clouds are used to extract foreground objects, which are then upsampled to create an object library. Objects are dynamically introduced into the LiDAR point cloud during training. Parameters similar to actual LiDAR settings are used to mimic realistic scanning lines and shadows by using range images, which are generated depending on the resolution of the LiDAR used for data gathering.
    \item \textbf{PointWOLF} : To enhance the variation of known 3D objects, this approach \parencite{pointwolf_2021} uses weighted alterations within local neighborhoods. It might be used, for example, on objects like wolves to produce different poses. The needed diversity is obtained by carefully combining several local transformations that are done with regard to anchor points in smoothly varied ways.
    \item \textbf{Real3D-Aug} : In this method \parencite{sebek2022real3daug}, objects are strategically placed at the same distance with identical observation angles to ensure consistent object point density and LiDAR intensity across the dataset. Additionally, collision avoidance measures are implemented by analyzing overlapping bounding boxes within semantic datasets.
    \item \textbf{Contextual Ground Truth Sampling} : In order to rectify data imbalances and insert ground truth \acrshort{gt} objects in realistic placements, this method \parencite{lee2022resolving} uses contextual GT sampling (sampling ground truth objects from a pre-saved GT database).
\end{itemize}

\section{Need of the Thesis work}

The majority of augmentation methods primarily aim to enhance model performance for detecting specific classes using pre-labeled class instances. Many existing strategies assume prior knowledge of ground truth objects, with some leveraging state-of-the-art semantic models to obtain this information \parencite{rs_aug_2023}. However, occlusions are often overlooked in some augmentation strategies \parencite{second2018, xiao2022polarmix}, and accurate 3D bounding boxes are typically required for appropriate object insertion into the scene. While some approaches utilize a database of rendered graphics models of objects \parencite{second2018, rs_aug_2023}, simulators often rely on CAD models, which may exhibit unrealistic appearances, necessitate costly manual construction, and lack scalability to represent the diversity and complexity of real-world scenarios. Additionally, certain augmentation methods require knowledge of the LiDAR configurations used for the target dataset to facilitate augmentation effectively \parencite{9811816, dada_2023}. 
Semantic segmentation of LiDAR frequently exhibits a decrease in accuracy when applied to LiDAR setups not previously encountered, as elucidated in \parencite{10204248}. This discrepancy results in an accuracy decline in LiDAR data-based semantic classification tasks when the LiDAR operated for gathering the evaluation set contradicts the LiDAR utilized for the learning set. The variance in sampling patterns across various LiDAR configurations, characterized by differences in vertical and horizontal resolution, contributes to the emergence of sensor bias issues in 3D perception algorithms \parencite{survery_domain_adp_2021, domain_adp_appr_2020}. 
Certain augmentation techniques, like PA-AUG and PointWolf, concentrate only on enhancing the ground truth (GT) objects, they do not handle how these objects are combined with the surrounding scene. 
Current augmentation strategies rely on either known labeled data or semantic models to extract ground truth objects. However, there could be situations where the label of the ground truth objects to be extracted is unknown. \acrshort{sota} semantic models may also prove ineffective, as the model may not have encountered the specific object during the training.
Even though the 3D semantic models have been trained on relevant objects before, using 3D semantic models is still ineffective as it requires an ideal model for the extraction of the specific object and the present \acrshort{sota} 3D Semantic Models performance (Mean \acrshort{iou}) on real-world datasets like SemanticKITTI is below 80\% \parencite{papers-with-code}.
This highlights the need for alternative approaches to handle scenarios where ground truth information is unavailable or difficult to obtain.

