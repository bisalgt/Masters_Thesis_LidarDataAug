\chapter{Introduction}

\section{Motivation}
A survey report (National Motor Vehicle Crash Causation Survey) presented by the  U.S. Department of
Transportation, National Highway Traffic Safety Administration in 2008 \parencite{nmvccs} shows that about \(94\%\) of road accidents are caused by human errors. Failure to use a seatbelt, excessive speeding, distractions from various sources, and tailgating are among the causes of road incidents. False assumptions about other drivers' habits and neglecting to check traffic before pulling out also contribute to these incidents. With the goal of preventing collisions, cutting emissions, assisting the mobility-impaired in getting around, and easing the stress associated with driving, \acrfull{had} Systems are being researched. The development of deep learning and the accessibility of sensors such as lidar, radar, camera, etc have led to significant advancements in computer vision and accelerated the study and application of \acrshort{had} in industry. Autonomous robotaxis are operating in major cities like San Francisco, Phoenix, Beijing, Shanghai, etc \parencite{robotaxis}. Among the sensors such as camera, ultrasonic, radar, and lidar, the depth information is captured by all the sensors except camera. Among the depth information capturing sensors, lidar has the highest accuracy for a range less than 200 meters \parencite{Yurtsever_2020}. Some features of lidar include high depth perception accuracy and resolution, sturdy under dim light, and capable of velocity measurement, etc. However, companies have different carlines and the setup for each carline is different. For detection in short to middle range, cameras are dominantly being used (e.g. Tesla Carlines, Mercedes Class S) in the \acrshort{had} industry. Lidar and Radar are used in carlines mostly for middle to long-range detection. A Lidar-equipped Lexus and a Tesla Model 3 were tested in the dark by Luminar to demonstrate the variations in safety measures \parencite{tesla_vs_lexus}. They encountered a dummy pedestrian. The Tesla mowed down the dummy while the Lexus stopped in time. While the system's usefulness cannot be disputed, this seems to indicate that the camera-equipped Tesla is less effective at night. The 2007 DARPA Grand Challenge showcased the capabilities of LiDAR perception systems and was a significant event in the field of autonomous driving. Each of the top three teams' systems was harnessed with LiDAR. Since 2015 \parencite{guo2020deep}, deep learning-based point cloud analysis, including object categorization and segmentation, has advanced significantly thanks to the availability of publically available datasets and progress in deep learning methodology for 3D. Deep learning on 3D point clouds is still beset by a number of serious difficulties, including the chaotic structure of 3D point clouds, the high dimensionality, and the small size of datasets \parencite{qi2017pointnet}. In the context of lidar sensors like Velodyne HDL-64E, the sensor reads more than 1.3 million points per second from space giving an output of 100MBPS UDP Packets \parencite{velodyne_64}. Annotating and verifying each and every point of the lidar point cloud is time-consuming. It took annotators about 1700 hours to label and verify the labeling process in the SemanticKITTI dataset \parencite{behley2019semantickitti}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{97_graphics//introduction/naturalness_vs_density.pdf}
    \caption{Images from KITTI Vision Benchmark Suite \parencite{Geiger2012CVPR}}
    \label{fig:introduction_surf_variation_in_kitti}
\end{figure}

Figure \ref{fig:introduction_surf_variation_in_kitti}, shows a brief look into the diversity of data in SemanticKITTI. It is understood from the figure that most of the data in SemanticKITTI represents either downtown or natural vegetation. So there lies a question of how to close the gap in \acrfull{sota} datasets like SemanticKITTI that is used as a base for semantic understanding of \acrshort{had} system benchmarking \parencite{papers-with-code}. Because of the interactions between vehicles driven by humans, the chances of accidents are low. As a result, the dataset collected in an open-world environment may not represent complete possible scenarios. Such conditions lead to faulty tests of \acrshort{had} systems. For example in 2022, Cruise Robotaxis had 546,492 driverless miles in California and even though the number of miles driven by the autonomous vehicles of Cruise in California for 2023 was a total of 583,624 Miles with 510 Vehicles \parencite{disengagement_report}, accident occurred on February 10, 2023. A woman was struck by a human-driven Nissan at a red light, which caused her to fall under the passenger side of the autonomous Cruise Robotaxi that was moving parallel to the Nissan. When the cab attempted to pull over to the edge of the road, it dragged the woman a few meters instead of stopping instantly \parencite{cruise_crash}. Even though the Cruise Robotaxis had driven millions of miles in the real world as \acrshort{had} system, such a horrific situation occurred due to the result of a scenario that was never seen by the Cruise Robotaxis. Deficits of the variety of scenarios (positions, behavior or pose, the appearance of pedestrians, and also the scenery like the background or surroundings of pedestrians) necessitate a new method of scenario generation that represents the real world better than the synthetic simulations and can be used to test the \acrshort{had} systems effectiveness and harmlessness.

\section{Applications}
Annotated data is necessary to apply deep learning technology. Deep neural network models are trained, validated, and tested using the known labels. A predominant strategy for mitigating the workload associated with data annotation involves the generation of synthetic data through environmental simulations. This approach exhibits potential by streamlining the acquisition of all ground-truth labels. Even though the performance of the object detection models is improved \parencite{johnsonroberson2017driving}, there still exists a domain gap between the synthetic and real-world data \parencite{care_real_and_syn_gap}. There exists a question about the fidelity of sensors in a simulation involving lidars because of reasons such as unreturned pulses, multiple echos from the environment, retroreflectors, spurious returns, etc \parencite{zero_domain_gap}. As a result of these factors, achieving high-fidelity modeling of simulators involving LiDAR sensors remains unattained. These factors have prompted the utilization of simulations, such as \acrfull{mil}, \acrfull{sil}, \acrfull{hil}, \acrfull{vil}, etc during the early phases of \acrshort{had} system development and research \parencite{x_in_loop}. Another strategy to deal with limited annotated data is data augmentation \parencite{lidar_aug}. We ask the research question : 
\begin{itemize}
    \item How to reduce the gap between the simulation and the real world?
    \item How to remove the deficiency in scenarios?
\end{itemize}


